{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b41648b4",
   "metadata": {},
   "source": [
    "# 1. Haar Cascade(face detect) + Dlib(eye landmark) + EAR algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda9e8f8",
   "metadata": {},
   "source": [
    "### Haar Cascade를 이용해 얼굴 영역을 더 빠르게 찾을 수 있으나 인식률이 떨어짐\n",
    "### 특히 측면 얼굴은 인식하지 못함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a55773ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame time 0.232 seconds\n",
      "Frame time 0.199 seconds\n",
      "Frame time 0.199 seconds\n",
      "Frame time 0.219 seconds\n",
      "Frame time 0.220 seconds\n",
      "Frame time 0.230 seconds\n",
      "Frame time 0.213 seconds\n",
      "Frame time 0.186 seconds\n",
      "Frame time 0.215 seconds\n",
      "Frame time 0.227 seconds\n",
      "Frame time 0.209 seconds\n",
      "Frame time 0.185 seconds\n",
      "Frame time 0.186 seconds\n",
      "Frame time 0.196 seconds\n",
      "Frame time 0.201 seconds\n",
      "Frame time 0.200 seconds\n",
      "Frame time 0.202 seconds\n",
      "Frame time 0.223 seconds\n",
      "Frame time 0.212 seconds\n",
      "Frame time 0.182 seconds\n",
      "Frame time 0.205 seconds\n",
      "Frame time 0.194 seconds\n",
      "Frame time 0.216 seconds\n",
      "Frame time 0.216 seconds\n",
      "Frame time 0.208 seconds\n",
      "Frame time 0.218 seconds\n",
      "Frame time 0.132 seconds\n",
      "Frame time 0.126 seconds\n",
      "Frame time 0.127 seconds\n",
      "Frame time 0.136 seconds\n",
      "Frame time 0.128 seconds\n",
      "Frame time 0.142 seconds\n",
      "Frame time 0.146 seconds\n",
      "Frame time 0.138 seconds\n",
      "Frame time 0.141 seconds\n",
      "Frame time 0.148 seconds\n",
      "Frame time 0.131 seconds\n",
      "Frame time 0.148 seconds\n",
      "Frame time 0.149 seconds\n",
      "Frame time 0.152 seconds\n",
      "Frame time 0.156 seconds\n",
      "Frame time 0.148 seconds\n",
      "Frame time 0.147 seconds\n",
      "Frame time 0.142 seconds\n",
      "Frame time 0.153 seconds\n",
      "Frame time 0.156 seconds\n",
      "Frame time 0.137 seconds\n",
      "Frame time 0.146 seconds\n",
      "Frame time 0.149 seconds\n",
      "Frame time 0.143 seconds\n",
      "Frame time 0.136 seconds\n",
      "Frame time 0.154 seconds\n",
      "Frame time 0.131 seconds\n",
      "Frame time 0.136 seconds\n",
      "Frame time 0.148 seconds\n",
      "Frame time 0.151 seconds\n",
      "Frame time 0.148 seconds\n",
      "Frame time 0.145 seconds\n",
      "Frame time 0.134 seconds\n",
      "Frame time 0.137 seconds\n",
      "Frame time 0.138 seconds\n",
      "Frame time 0.148 seconds\n",
      "Frame time 0.129 seconds\n",
      "Frame time 0.135 seconds\n",
      "Frame time 0.138 seconds\n",
      "Frame time 0.133 seconds\n",
      "Frame time 0.136 seconds\n",
      "Frame time 0.142 seconds\n",
      "Frame time 0.118 seconds\n",
      "Frame time 0.140 seconds\n",
      "Frame time 0.130 seconds\n",
      "Frame time 0.135 seconds\n",
      "Frame time 0.124 seconds\n",
      "Frame time 0.140 seconds\n",
      "Frame time 0.111 seconds\n",
      "Frame time 0.133 seconds\n",
      "Frame time 0.134 seconds\n",
      "Frame time 0.128 seconds\n",
      "Frame time 0.126 seconds\n",
      "Frame time 0.125 seconds\n",
      "Frame time 0.125 seconds\n",
      "Frame time 0.132 seconds\n",
      "Frame time 0.134 seconds\n",
      "Frame time 0.131 seconds\n",
      "Frame time 0.129 seconds\n",
      "Frame time 0.143 seconds\n",
      "Frame time 0.127 seconds\n",
      "Frame time 0.148 seconds\n",
      "Frame time 0.138 seconds\n",
      "Frame time 0.153 seconds\n",
      "Frame time 0.166 seconds\n",
      "Frame time 0.161 seconds\n",
      "Frame time 0.140 seconds\n",
      "Frame time 0.155 seconds\n",
      "Frame time 0.145 seconds\n",
      "Frame time 0.151 seconds\n",
      "Frame time 0.149 seconds\n",
      "Frame time 0.147 seconds\n",
      "Frame time 0.147 seconds\n",
      "Frame time 0.130 seconds\n",
      "Frame time 0.128 seconds\n",
      "Frame time 0.141 seconds\n",
      "Frame time 0.146 seconds\n",
      "Frame time 0.127 seconds\n",
      "Frame time 0.126 seconds\n",
      "Frame time 0.145 seconds\n",
      "Frame time 0.128 seconds\n",
      "Frame time 0.142 seconds\n",
      "Frame time 0.145 seconds\n",
      "Frame time 0.137 seconds\n",
      "Frame time 0.124 seconds\n",
      "Frame time 0.134 seconds\n",
      "Frame time 0.132 seconds\n",
      "Frame time 0.128 seconds\n",
      "Frame time 0.131 seconds\n",
      "Frame time 0.134 seconds\n",
      "Frame time 0.143 seconds\n",
      "Frame time 0.124 seconds\n",
      "Frame time 0.130 seconds\n",
      "Frame time 0.131 seconds\n",
      "Frame time 0.141 seconds\n",
      "Frame time 0.125 seconds\n",
      "Frame time 0.121 seconds\n",
      "Frame time 0.135 seconds\n",
      "Frame time 0.139 seconds\n",
      "Frame time 0.131 seconds\n",
      "Frame time 0.167 seconds\n",
      "Frame time 0.166 seconds\n",
      "Frame time 0.158 seconds\n",
      "Frame time 0.154 seconds\n",
      "Frame time 0.149 seconds\n",
      "Frame time 0.154 seconds\n",
      "Frame time 0.146 seconds\n",
      "Frame time 0.146 seconds\n",
      "Frame time 0.144 seconds\n",
      "Frame time 0.160 seconds\n",
      "Frame time 0.156 seconds\n",
      "Frame time 0.158 seconds\n",
      "Frame time 0.146 seconds\n",
      "Frame time 0.162 seconds\n",
      "Frame time 0.154 seconds\n",
      "Frame time 0.166 seconds\n",
      "Frame time 0.150 seconds\n",
      "Frame time 0.151 seconds\n",
      "Frame time 0.154 seconds\n",
      "Frame time 0.151 seconds\n",
      "Frame time 0.160 seconds\n",
      "Frame time 0.148 seconds\n",
      "Frame time 0.163 seconds\n",
      "Frame time 0.150 seconds\n",
      "Frame time 0.160 seconds\n",
      "Frame time 0.149 seconds\n",
      "Frame time 0.147 seconds\n",
      "Frame time 0.155 seconds\n",
      "Frame time 0.163 seconds\n",
      "Frame time 0.166 seconds\n",
      "Frame time 0.153 seconds\n",
      "Frame time 0.152 seconds\n",
      "Frame time 0.162 seconds\n",
      "Frame time 0.148 seconds\n",
      "Frame time 0.154 seconds\n",
      "Frame time 0.151 seconds\n",
      "Frame time 0.156 seconds\n",
      "Frame time 0.154 seconds\n",
      "Frame time 0.168 seconds\n",
      "Frame time 0.154 seconds\n",
      "Frame time 0.166 seconds\n",
      "Frame time 0.161 seconds\n",
      "Frame time 0.157 seconds\n",
      "Frame time 0.160 seconds\n",
      "Frame time 0.164 seconds\n",
      "Frame time 0.172 seconds\n",
      "Frame time 0.154 seconds\n",
      "Frame time 0.160 seconds\n",
      "Frame time 0.148 seconds\n",
      "Frame time 0.153 seconds\n",
      "Frame time 0.158 seconds\n",
      "Frame time 0.239 seconds\n",
      "Frame time 0.173 seconds\n",
      "Frame time 0.168 seconds\n",
      "Frame time 0.151 seconds\n",
      "Frame time 0.173 seconds\n",
      "Frame time 0.145 seconds\n",
      "Frame time 0.169 seconds\n",
      "Frame time 0.141 seconds\n",
      "Frame time 0.132 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import dlib\n",
    "import cv2\n",
    "import time\n",
    "#import pygame\n",
    "\n",
    "#pygame.mixer.init()\n",
    "#pygame.mixer.music.load('./audio/fire-truck.wav')\n",
    "\n",
    "RIGHT_EYE = list(range(36, 42))\n",
    "LEFT_EYE = list(range(42, 48))\n",
    "EYES = list(range(36, 48))\n",
    "frame_width = 640\n",
    "frame_height = 480\n",
    "\n",
    "title_name = 'Face Drowsiness Detection'\n",
    "elapsed_time = 0\n",
    "\n",
    "face_cascade_name = cv2.data.haarcascades + 'haarcascade_frontalface_alt.xml'\n",
    "face_cascade = cv2.CascadeClassifier()\n",
    "if not face_cascade.load(cv2.samples.findFile(face_cascade_name)):\n",
    "    print('Error loading face cascade')\n",
    "    exit(0)\n",
    "\n",
    "predictor_file = 'study/model/shape_predictor_68_face_landmarks.dat'\n",
    "predictor = dlib.shape_predictor(predictor_file)\n",
    "\n",
    "status = 'Awake'\n",
    "number_closed = 0\n",
    "min_EAR = 0.25\n",
    "closed_limit = 7\n",
    "show_frame = None\n",
    "sign = None\n",
    "color = None\n",
    "\n",
    "def getEAR(points): #EAR 값을 구하는 함수\n",
    "    A = np.linalg.norm(points[1] - points[5])\n",
    "    B = np.linalg.norm(points[2] - points[4])\n",
    "    C = np.linalg.norm(points[0] - points[3])\n",
    "    return (A + B) / (2.0 * C)\n",
    "    \n",
    "def detectAndDisplay(image):\n",
    "    global number_closed\n",
    "    global color\n",
    "    global show_frame\n",
    "    global sign\n",
    "    global elapsed_time\n",
    "    start_time = time.time()\n",
    "    #height,width = image.shape[:2]\n",
    "    image = cv2.resize(image, (frame_width, frame_height))\n",
    "    show_frame = image\n",
    "    frame_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #인식률을 높이기 위해 noise를 없앰\n",
    "    frame_gray = cv2.equalizeHist(frame_gray)\n",
    "    faces = face_cascade.detectMultiScale(frame_gray) #haar cascade를 이용해 face detect\n",
    "    \n",
    "    for (x,y,w,h) in faces: #haar cascade가 찾은 좌표\n",
    "        cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        \n",
    "        rect = dlib.rectangle(int(x), int(y), int(x + w), int(y + h))\n",
    "        points = np.matrix([[p.x, p.y] for p in predictor(frame_gray, rect).parts()])\n",
    "        show_parts = points[EYES]\n",
    "        right_eye_EAR = getEAR(points[RIGHT_EYE]) #EAR 값 구함\n",
    "        left_eye_EAR = getEAR(points[LEFT_EYE])\n",
    "        mean_eye_EAR = (right_eye_EAR + left_eye_EAR) / 2 \n",
    "\n",
    "        right_eye_center = np.mean(points[RIGHT_EYE], axis = 0).astype(\"int\")\n",
    "        left_eye_center = np.mean(points[LEFT_EYE], axis = 0).astype(\"int\")\n",
    "\n",
    "        cv2.putText(image, \"{:.2f}\".format(right_eye_EAR), (right_eye_center[0,0], right_eye_center[0,1] + 20),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "        cv2.putText(image, \"{:.2f}\".format(left_eye_EAR), (left_eye_center[0,0], left_eye_center[0,1] + 20),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "        \n",
    "        for (i, point) in enumerate(show_parts): #눈의 landmark 표시\n",
    "            x = point[0,0]\n",
    "            y = point[0,1]\n",
    "            cv2.circle(image, (x, y), 1, (0, 255, 255), -1)\n",
    "            \n",
    "        if mean_eye_EAR > min_EAR: #눈을 떴다고 판단\n",
    "            color = (0, 255, 0)\n",
    "            status = 'Awake'\n",
    "            number_closed = number_closed - 1\n",
    "            if( number_closed<0 ):\n",
    "                number_closed = 0\n",
    "        else: #눈을 감았다고 판단\n",
    "            color = (0, 0, 255)\n",
    "            status = 'Sleep'\n",
    "            number_closed = number_closed + 1\n",
    "                     \n",
    "        sign = status + ', Sleep count : ' + str(number_closed) + ' / ' + str(closed_limit)\n",
    "        if( number_closed > closed_limit ):\n",
    "            show_frame = frame_gray\n",
    "            # play SOUND\n",
    "            #if(pygame.mixer.music.get_busy()==False):\n",
    "                #pygame.mixer.music.play()\n",
    "\n",
    "    cv2.putText(show_frame, sign , (10,frame_height-20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "    cv2.imshow(title_name, show_frame)\n",
    "    frame_time = time.time() - start_time\n",
    "    elapsed_time += frame_time\n",
    "    print(\"Frame time {:.3f} seconds\".format(frame_time))\n",
    "    \n",
    "\n",
    "vs = cv2.VideoCapture(0)\n",
    "time.sleep(2.0)\n",
    "if not vs.isOpened:\n",
    "    print('Error opening video')\n",
    "    exit(0)\n",
    "while True:\n",
    "    ret, frame = vs.read()\n",
    "    if frame is None:\n",
    "        print('No more frame')\n",
    "        vs.release()\n",
    "        break\n",
    "    detectAndDisplay(frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "vs.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167a0594",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Dlib(face landmark + eye landmark) + EAR algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d383f86c",
   "metadata": {},
   "source": [
    "### 이전보다 측면의 눈을 잘 인식함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95be89d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame time 0.728 seconds\n",
      "Frame time 0.615 seconds\n",
      "Frame time 0.559 seconds\n",
      "Frame time 0.563 seconds\n",
      "Frame time 0.588 seconds\n",
      "Frame time 0.614 seconds\n",
      "Frame time 0.634 seconds\n",
      "Frame time 0.544 seconds\n",
      "Frame time 0.514 seconds\n",
      "Frame time 0.415 seconds\n",
      "Frame time 0.352 seconds\n",
      "Frame time 0.327 seconds\n",
      "Frame time 0.287 seconds\n",
      "Frame time 0.321 seconds\n",
      "Frame time 0.306 seconds\n",
      "Frame time 0.306 seconds\n",
      "Frame time 0.295 seconds\n",
      "Frame time 0.302 seconds\n",
      "Frame time 0.322 seconds\n",
      "Frame time 0.322 seconds\n",
      "Frame time 0.380 seconds\n",
      "Frame time 0.364 seconds\n",
      "Frame time 0.386 seconds\n",
      "Frame time 0.320 seconds\n",
      "Frame time 0.444 seconds\n",
      "Frame time 0.608 seconds\n",
      "Frame time 0.525 seconds\n",
      "Frame time 0.554 seconds\n",
      "Frame time 0.525 seconds\n",
      "Frame time 0.479 seconds\n",
      "Frame time 0.489 seconds\n",
      "Frame time 0.521 seconds\n",
      "Frame time 0.328 seconds\n",
      "Frame time 0.300 seconds\n",
      "Frame time 0.317 seconds\n",
      "Frame time 0.290 seconds\n",
      "Frame time 0.292 seconds\n",
      "Frame time 0.289 seconds\n",
      "Frame time 0.301 seconds\n",
      "Frame time 0.318 seconds\n",
      "Frame time 0.307 seconds\n",
      "Frame time 0.308 seconds\n",
      "Frame time 0.309 seconds\n",
      "Frame time 0.295 seconds\n",
      "Frame time 0.303 seconds\n",
      "Frame time 0.317 seconds\n",
      "Frame time 0.289 seconds\n",
      "Frame time 0.306 seconds\n",
      "Frame time 0.321 seconds\n",
      "Frame time 0.325 seconds\n",
      "Frame time 0.295 seconds\n",
      "Frame time 0.319 seconds\n",
      "Frame time 0.307 seconds\n",
      "Frame time 0.299 seconds\n",
      "Frame time 0.304 seconds\n",
      "Frame time 0.289 seconds\n",
      "Frame time 0.363 seconds\n",
      "Frame time 0.314 seconds\n",
      "Frame time 0.341 seconds\n",
      "Frame time 0.285 seconds\n",
      "Frame time 0.306 seconds\n",
      "Frame time 0.332 seconds\n",
      "Frame time 0.311 seconds\n",
      "Frame time 0.294 seconds\n",
      "Frame time 0.291 seconds\n",
      "Frame time 0.299 seconds\n",
      "Frame time 0.290 seconds\n",
      "Frame time 0.325 seconds\n",
      "Frame time 0.314 seconds\n",
      "Frame time 0.301 seconds\n",
      "Frame time 0.291 seconds\n",
      "Frame time 0.320 seconds\n",
      "Frame time 0.298 seconds\n",
      "Frame time 0.281 seconds\n",
      "Frame time 0.302 seconds\n",
      "Frame time 0.298 seconds\n",
      "Frame time 0.313 seconds\n",
      "Frame time 0.317 seconds\n",
      "Frame time 0.307 seconds\n",
      "Frame time 0.294 seconds\n",
      "Frame time 0.312 seconds\n",
      "Frame time 0.315 seconds\n",
      "Frame time 0.301 seconds\n",
      "Frame time 0.312 seconds\n",
      "Frame time 0.306 seconds\n",
      "Frame time 0.299 seconds\n",
      "Frame time 0.309 seconds\n",
      "Frame time 0.337 seconds\n",
      "Frame time 0.297 seconds\n",
      "Frame time 0.303 seconds\n",
      "Frame time 0.324 seconds\n",
      "Frame time 0.292 seconds\n",
      "Frame time 0.288 seconds\n",
      "Frame time 0.300 seconds\n",
      "Frame time 0.311 seconds\n",
      "Frame time 0.313 seconds\n",
      "Frame time 0.315 seconds\n",
      "Frame time 0.301 seconds\n",
      "Frame time 0.332 seconds\n",
      "Frame time 0.297 seconds\n",
      "Frame time 0.295 seconds\n",
      "Frame time 0.298 seconds\n",
      "Frame time 0.311 seconds\n",
      "Frame time 0.305 seconds\n",
      "Frame time 0.295 seconds\n",
      "Frame time 0.334 seconds\n",
      "Frame time 0.327 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import dlib\n",
    "import cv2\n",
    "import time\n",
    "#import pygame\n",
    "\n",
    "#pygame.mixer.init()\n",
    "#pygame.mixer.music.load('./audio/fire-truck.wav')\n",
    "\n",
    "RIGHT_EYE = list(range(36, 42))\n",
    "LEFT_EYE = list(range(42, 48))\n",
    "EYES = list(range(36, 48))\n",
    "frame_width = 640\n",
    "frame_height = 480\n",
    "\n",
    "title_name = 'Face Drowsiness Detection'\n",
    "elapsed_time = 0\n",
    "\n",
    "# face_cascade_name = cv2.data.haarcascades + 'haarcascade_frontalface_alt.xml'\n",
    "# face_cascade = cv2.CascadeClassifier()\n",
    "# if not face_cascade.load(cv2.samples.findFile(face_cascade_name)):\n",
    "#     print('Error loading face cascade')\n",
    "#     exit(0)\n",
    "\n",
    "predictor_file = 'study/model/shape_predictor_68_face_landmarks.dat'\n",
    "predictor = dlib.shape_predictor(predictor_file)\n",
    "\n",
    "####\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "status = 'Awake'\n",
    "number_closed = 0\n",
    "min_EAR = 0.25\n",
    "closed_limit = 7\n",
    "show_frame = None\n",
    "sign = None\n",
    "color = None\n",
    "\n",
    "def getEAR(points): #EAR 값을 구하는 함수\n",
    "    A = np.linalg.norm(points[1] - points[5])\n",
    "    B = np.linalg.norm(points[2] - points[4])\n",
    "    C = np.linalg.norm(points[0] - points[3])\n",
    "    return (A + B) / (2.0 * C)\n",
    "    \n",
    "def detectAndDisplay(image):\n",
    "    global number_closed\n",
    "    global color\n",
    "    global show_frame\n",
    "    global sign\n",
    "    global elapsed_time\n",
    "    start_time = time.time()\n",
    "    #height,width = image.shape[:2]\n",
    "    image = cv2.resize(image, (frame_width, frame_height))\n",
    "    show_frame = image\n",
    "    frame_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #인식률을 높이기 위해 noise를 없앰\n",
    "    frame_gray = cv2.equalizeHist(frame_gray)\n",
    "    faces = detector(frame_gray, 1) #haar cascade를 이용해 face detect\n",
    "    \n",
    "    for (i, face) in enumerate(faces): #haar cascade가 찾은 좌표\n",
    "#         cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        \n",
    "#         rect = dlib.rectangle(int(x), int(y), int(x + w), int(y + h))\n",
    "#         points = np.matrix([[p.x, p.y] for p in predictor(frame_gray, rect).parts()])\n",
    "#         show_parts = points[EYES]\n",
    "        points = np.matrix([[p.x, p.y] for p in predictor(frame_gray, face).parts()])\n",
    "        show_parts = points[EYES]\n",
    "        right_eye_EAR = getEAR(points[RIGHT_EYE]) #EAR 값 구함\n",
    "        left_eye_EAR = getEAR(points[LEFT_EYE])\n",
    "        mean_eye_EAR = (right_eye_EAR + left_eye_EAR) / 2 \n",
    "\n",
    "        right_eye_center = np.mean(points[RIGHT_EYE], axis = 0).astype(\"int\")\n",
    "        left_eye_center = np.mean(points[LEFT_EYE], axis = 0).astype(\"int\")\n",
    "\n",
    "        cv2.putText(image, \"{:.2f}\".format(right_eye_EAR), (right_eye_center[0,0], right_eye_center[0,1] + 20),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "        cv2.putText(image, \"{:.2f}\".format(left_eye_EAR), (left_eye_center[0,0], left_eye_center[0,1] + 20),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "        \n",
    "        for (i, point) in enumerate(show_parts): #눈의 landmark 표시\n",
    "            x = point[0,0]\n",
    "            y = point[0,1]\n",
    "            cv2.circle(image, (x, y), 1, (0, 255, 255), -1)\n",
    "            \n",
    "        if mean_eye_EAR > min_EAR: #눈을 떴다고 판단\n",
    "            color = (0, 255, 0)\n",
    "            status = 'Awake'\n",
    "            number_closed = number_closed - 1\n",
    "            if( number_closed<0 ):\n",
    "                number_closed = 0\n",
    "        else: #눈을 감았다고 판단\n",
    "            color = (0, 0, 255)\n",
    "            status = 'Sleep'\n",
    "            number_closed = number_closed + 1\n",
    "                     \n",
    "        sign = status + ', Sleep count : ' + str(number_closed) + ' / ' + str(closed_limit)\n",
    "        if( number_closed > closed_limit ):\n",
    "            show_frame = frame_gray\n",
    "            # play SOUND\n",
    "            #if(pygame.mixer.music.get_busy()==False):\n",
    "                #pygame.mixer.music.play()\n",
    "\n",
    "    cv2.putText(show_frame, sign , (10,frame_height-20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "    cv2.imshow(title_name, show_frame)\n",
    "    frame_time = time.time() - start_time\n",
    "    elapsed_time += frame_time\n",
    "    print(\"Frame time {:.3f} seconds\".format(frame_time))\n",
    "    \n",
    "\n",
    "vs = cv2.VideoCapture(0)\n",
    "time.sleep(2.0)\n",
    "if not vs.isOpened:\n",
    "    print('Error opening video')\n",
    "    exit(0)\n",
    "while True:\n",
    "    ret, frame = vs.read()\n",
    "    if frame is None:\n",
    "        print('No more frame')\n",
    "        vs.release()\n",
    "        break\n",
    "    detectAndDisplay(frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "vs.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aab1d9",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Dlib(face landmark + eye landmark) + tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b9b2fd",
   "metadata": {},
   "source": [
    "### open eyes, closed eyes 의 이미지(약 3000개)를 학습\n",
    "### 1. model 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bd07dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import dlib\n",
    "import time\n",
    "#deep learning model for training\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "Datadirectory = \"Train_Dataset/\" #training dataset\n",
    "Classes = [\"closed_eye\", \"open_eye\"]\n",
    "for category in Classes:\n",
    "    path = os.path.join(Datadirectory, category)\n",
    "    for img in os.listdir(path):\n",
    "        img_array = cv2.imread(os.path.join(path,img), cv2.IMREAD_GRAYSCALE)\n",
    "        backtorgb = cv2.cvtColor(img_array, cv2.COLOR_GRAY2RGB)\n",
    "#         plt.imshow(img_array, cmap=\"gray\")\n",
    "#         plt.show()\n",
    "        break\n",
    "    break\n",
    "\n",
    "img_size = 224\n",
    "new_array = cv2.resize(backtorgb, (img_size, img_size))\n",
    "# plt.imshow(new_array, cmap=\"gray\")\n",
    "# plt.show()\n",
    "\n",
    "#reading all the images and converting them into an array for data and labels\n",
    "training_Data = []\n",
    "def create_training_Data():\n",
    "    for category in Classes:\n",
    "        path = os.path.join(Datadirectory, category)\n",
    "        class_num = Classes.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
    "                backtorgb = cv2.cvtColor(img_array, cv2.COLOR_GRAY2RGB)\n",
    "                new_array = cv2.resize(backtorgb, (img_size, img_size))\n",
    "                training_Data.append([new_array,class_num])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "create_training_Data()\n",
    "\n",
    "import random\n",
    "random.shuffle(training_Data)\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for features,label in training_Data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "X = np.array(X).reshape(-1, img_size, img_size, 3)\n",
    "\n",
    "#normalize the data\n",
    "X = X/255.0\n",
    "\n",
    "Y = np.array(y)\n",
    "\n",
    "pickle_out = open(\"X.pickle\",\"wb\")\n",
    "pickle.dump(X, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"y.pickle\",\"wb\")\n",
    "pickle.dump(y, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "model = tf.keras.applications.mobilenet.MobileNet()\n",
    "\n",
    "# Trnasfer learning\n",
    "base_input = model.layers[0].input\n",
    "\n",
    "base_output = model.layers[-4].output\n",
    "\n",
    "Flat_layer = layers.Flatten()(base_output)\n",
    "final_output = layers.Dense(1)(Flat_layer) #one node\n",
    "final_output = layers.Activation('sigmoid')(final_output)\n",
    "\n",
    "new_model = keras.Model(inputs = base_input, outputs = final_output)\n",
    "\n",
    "#settings for binary classification\n",
    "new_model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "\n",
    "new_model.fit(X,Y,epochs=1,validation_split=0.1)\n",
    "\n",
    "new_model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d42bf4",
   "metadata": {},
   "source": [
    "### 2. model 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e822d1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 753ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import dlib\n",
    "import time\n",
    "#deep learning model for training\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "new_model = tf.keras.models.load_model('my_model.h5')\n",
    "frame_width = 640\n",
    "frame_height = 480\n",
    "\n",
    "predictor_path = \"study/model/shape_predictor_68_face_landmarks.dat\"\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "number_closed = 0\n",
    "\n",
    "path = \"haarcascade_frontalface_default.xml\"\n",
    "faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open webcam\")\n",
    "    \n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    #eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#     eyes = eye_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "#     for x,y,w,h in eyes:\n",
    "#         roi_gray = gray[y:y+h, x:x+w]\n",
    "#         roi_color = frame[y:y+h, x:x+w]\n",
    "#         cv2.rectangle(frame, (x, y), (x+w, y+h), (0,255,0), 2)\n",
    "#         eyess = eye_cascade.detectMultiScale(roi_gray)\n",
    "#         if len(eyess) == 0:\n",
    "#             print(\"eyes are not detected\")\n",
    "\n",
    "#         else:\n",
    "#             for (ex, ey, ew, eh) in eyess:\n",
    "#                 eyes_roi = roi_color[ey:ey+eh,ex:ex+ew]\n",
    "    dets = detector(gray)\n",
    "    for k, d in enumerate(dets):\n",
    "        shape = predictor(gray, d)\n",
    "    vec = np.empty([68, 2], dtype = int)\n",
    "    x1 = shape.part(36).x\n",
    "    x2 = shape.part(39).x\n",
    "    y1 = shape.part(36).y\n",
    "    img_crop = frame[y1-(x2-x1)//2:y1+(x2-x1)//2, x1:x2]\n",
    "    final_image = cv2.resize(img_crop, (224,224))\n",
    "    final_image = np.expand_dims(final_image,axis=0)\n",
    "    final_image=final_image/255.0\n",
    "    Predictions = new_model.predict(final_image)\n",
    "    #print(Predictions)0.00000001\n",
    "    if(Predictions>0.00000001):\n",
    "        status = \"Open Eyes\"\n",
    "    else:\n",
    "        status = \"Closed Eyes\"\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    #print(faceCascade.empty())\n",
    "    #faces = faceCascade.detectMultiScale(gray,1.1,4)\n",
    "    \n",
    "#     for(x,y,w,h) in faces:\n",
    "#         cv2.rectangle(frame, (x,y), (x+w, y+h), (0,255,0), 2)\n",
    "    \n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    \n",
    "    if status == \"Open Eyes\": #눈을 떴다고 판단\n",
    "        color = (0, 255, 0)\n",
    "        number_closed = number_closed - 1\n",
    "        if( number_closed<0 ):\n",
    "            number_closed = 0\n",
    "    else: #눈을 감았다고 판단\n",
    "        color = (0, 0, 255)\n",
    "        number_closed = number_closed + 1\n",
    "                     \n",
    "    sign = status + ', Sleep count : ' + str(number_closed) + ' / ' + str(7)\n",
    "#     if( number_closed > 7 ):\n",
    "#         show_frame = frame_gray\n",
    "            # play SOUND\n",
    "            #if(pygame.mixer.music.get_busy()==False):\n",
    "                #pygame.mixer.music.play()\n",
    "\n",
    "    cv2.putText(frame, sign , (10,frame_height-20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "    #cv2.imshow(title_name, frame)\n",
    "#     frame_time = time.time() - start_time\n",
    "#     elapsed_time += frame_time\n",
    "#     print(\"Frame time {:.3f} seconds\".format(frame_time))\n",
    "    \n",
    "    cv2.putText(frame, status, (50,50), font, 3, (0,0,255), 2, cv2.LINE_4)\n",
    "    cv2.imshow('Drowsiness Detection Tutorial', frame)\n",
    "    \n",
    "    if cv2.waitKey(2) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ee0d0e",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Haar cascde(face detect+eye detect) + Tensorflow\n",
    "### - 계획서에 썼던 내용\n",
    "### - Haar cascde 인식률이 좋지 않음\n",
    "### - tensorflow model은 위와 같은 모델을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9e8e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#real time video demo\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "#deep learning model for training\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "new_model = tf.keras.models.load_model('my_model.h5')\n",
    "\n",
    "path = \"haarcascade_frontalface_default.xml\"\n",
    "faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open webcam\")\n",
    "    \n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    eyes = eye_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "    for x,y,w,h in eyes:\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0,255,0), 2)\n",
    "        eyess = eye_cascade.detectMultiScale(roi_gray)\n",
    "        if len(eyess) == 0:\n",
    "            print(\"eyes are not detected\")\n",
    "\n",
    "        else:\n",
    "            for (ex, ey, ew, eh) in eyess:\n",
    "                eyes_roi = roi_color[ey:ey+eh,ex:ex+ew]\n",
    "    final_image = cv2.resize(eyes_roi, (224,224))\n",
    "    final_image = np.expand_dims(final_image,axis=0)\n",
    "    final_image=final_image/255.0\n",
    "    Predictions = new_model.predict(final_image)\n",
    "    if(Predictions>0.00000001):\n",
    "        status = \"Open Eyes\"\n",
    "    else:\n",
    "        status = \"Closed Eyes\"\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    print(faceCascade.empty())\n",
    "    faces = faceCascade.detectMultiScale(gray,1.1,4)\n",
    "    \n",
    "    for(x,y,w,h) in faces:\n",
    "        cv2.rectangle(frame, (x,y), (x+w, y+h), (0,255,0), 2)\n",
    "    \n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    \n",
    "    cv2.putText(frame, status, (50,50), font, 3, (0,0,255), 2, cv2.LINE_4)\n",
    "    cv2.imshow('Drowsiness Detection Tutorial', frame)\n",
    "    \n",
    "    if cv2.waitKey(2) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
